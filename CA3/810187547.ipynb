{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: Sajjad Alizadeh <br>\n",
    "#STD: 810197547"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\" style=\"color:black;font-size:18px;\">\n",
    "در همه حالات زیر ستون تیتر و ستون متن یکی در نظر گرفته شده است\n",
    "<br>\n",
    "    همچنین برای افزایش دقت از لگاریتم احتمال استفاده شده است.\n",
    "    <br>\n",
    "    تعریف prior:\n",
    "    <br>\n",
    "    احتمال اینکه یک کامنت پیشنهاد شده باشد یا نباشد بدون اینکه هیچ evidence ای موجود باشد. این احتمال برای حالت پیشنهاد شده برابر تعداد پیامهای پیشنهاد شده تقسیم بر کل پیامها و برای حالت پیشنهاد نشده برابر تعداد پیامهای پیشنهاد نشده تقسیم بر کل پیامها است\n",
    "    <br>\n",
    "    تعریف likelihood:\n",
    "    <br>\n",
    "    احتمال حضور یک پیام در یک کامنت به شرطی که پیشنهاد شده یا پیشنهاد نشده باشد. به عنوان مثال احتمال وجود کلمه «فدایی» در یک پیام به شرطی که آن پیام پیشنهاد شده باشد برابر تعداد تکرار این کلمه در کل پیامهای پیشنهاد شده تقسیم بر تعداد کل کلمات در کامنتهای پیشنهاد شده است.\n",
    "    <br>\n",
    "    تعریف evidence:\n",
    "    حضور هر کلمه در یک کامنت یک evidence است.\n",
    "    <br>\n",
    "     تعریف posterior:\n",
    "    <br>\n",
    "    احتمال اینکه یک کامنت پیشنهاد شده باشد یا نباشد به شرط مجموعه ای از evidence ها. که همانطور که ذکر شد evidence همان حضور کلمات است. به عبارت دیگر احتمال اینکه یک کامنت پیشنهاد شده باشد به شرطی که کلمات آن را بدانیم (که میدانیم) و با استفاده از قانون بیز و شبکه های بیزی این احتمال را محاسبه میکنیم و بر اساس آن تصمیم گیری میکنیم\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# No Pre Process, No Additive Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 90.625000\n",
      "Precision: 86.848073\n",
      "Recall: 95.750000\n",
      "F1: 91.082045\n"
     ]
    }
   ],
   "source": [
    "from __future__ import unicode_literals\n",
    "from hazm import *\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "REC = 0\n",
    "NOT_REC = 1\n",
    "trained = [dict(), dict()]\n",
    "\n",
    "def read_and_tokenize(path):\n",
    "    file_content = pd.read_csv(path)\n",
    "    file_content.replace('not_recommended', NOT_REC, inplace = True)\n",
    "    file_content.replace('recommended', REC, inplace = True)\n",
    "    train_data = file_content.values.tolist()\n",
    "    for i in train_data:\n",
    "        i[0] = word_tokenize(i[0])\n",
    "        i[1] = word_tokenize(i[1])\n",
    "        i[1] = i[1] + i[0]\n",
    "    return train_data\n",
    "\n",
    "train_data = read_and_tokenize(\"comment_train.csv\")\n",
    "\n",
    "size = [0, 0]\n",
    "for i in train_data:\n",
    "    rec_st = i[2]\n",
    "    for j in i[1]:\n",
    "        if j in trained[rec_st]: trained[rec_st][j] += 1\n",
    "        else: trained[rec_st][j] = 1\n",
    "        size[rec_st] += 1\n",
    "\n",
    "for i in trained[REC]:\n",
    "    trained[REC][i] = math.log(trained[REC][i] / size[REC])\n",
    "\n",
    "for i in trained[NOT_REC]:\n",
    "    trained[NOT_REC][i] = math.log(trained[NOT_REC][i] / size[NOT_REC])\n",
    "\n",
    "rec_notrec_prob = [math.log(size[REC] / (size[REC] + size[NOT_REC])), math.log(size[NOT_REC] / (size[REC] + size[NOT_REC]))]\n",
    "\n",
    "test_data = read_and_tokenize(\"comment_test.csv\")\n",
    "\n",
    "result = []\n",
    "for comment in test_data:\n",
    "    rec_prob = rec_notrec_prob[REC]\n",
    "    not_rec_prob = rec_notrec_prob[NOT_REC]\n",
    "    for word in comment[1]:\n",
    "        if (word in trained[REC]) and (word in trained[NOT_REC]):\n",
    "            rec_prob = rec_prob + trained[REC][word]\n",
    "            not_rec_prob = not_rec_prob + trained[NOT_REC][word]\n",
    "        elif (word in trained[REC]) and (word not in trained[NOT_REC]):\n",
    "            rec_prob = rec_prob + trained[REC][word]\n",
    "            not_rec_prob = float('-inf')\n",
    "        elif (word not in trained[REC]) and (word in trained[NOT_REC]):\n",
    "            rec_prob = float('-inf')\n",
    "            not_rec_prob = not_rec_prob + trained[NOT_REC][word]\n",
    "        elif (word not in trained[REC]) and (word not in trained[NOT_REC]):\n",
    "             rec_prob = rec_prob + math.log(0.5)\n",
    "             not_rec_prob = not_rec_prob + math.log(0.5)\n",
    "    if rec_prob < not_rec_prob: result.append(NOT_REC)\n",
    "    else: result.append(REC)\n",
    "        \n",
    "corrects = 0\n",
    "correct_decected_rec = 0\n",
    "all_detected_rec = 0\n",
    "total_rec = 0\n",
    "for i in range(len(test_data)):\n",
    "    if test_data[i][2] == result[i]:\n",
    "        corrects += 1\n",
    "    if result[i] == REC:\n",
    "        all_detected_rec += 1\n",
    "        if test_data[i][2] == REC:\n",
    "            correct_decected_rec += 1\n",
    "    if test_data[i][2] == REC:\n",
    "        total_rec += 1\n",
    "precision = (correct_decected_rec / all_detected_rec) * 100\n",
    "recall = (correct_decected_rec / total_rec) * 100\n",
    "print(\"Accuracy: %f\" % ((corrects / len(test_data)) * 100))\n",
    "print(\"Precision: %f\" % precision)\n",
    "print(\"Recall: %f\" % recall)\n",
    "print(\"F1: %f\" % ((2*precision*recall)/(precision+recall)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\" style=\"color:black;font-size:18px;\">\n",
    "معیار precision معیار خوبی برای مقایسه نیست. فرض کنید هزار داده داریم که پانصدتای آنها recommanded هستند و الگوریتم دویست تا از پانصدتا را recommanded در نظر بگیرد. در این صورت معیار precision آن برابر صد خواهد شد در صورتی که واقعا پیش بینی خوبی صورت نگرفته است. به عبارتی فقط یک پنجم داده ها درست پیش بینی شده اند.\n",
    "    <br>\n",
    "    recall نیز معیار خوبی برای مقایسه نیست. همان مثال بالا که پانصد کامنت recommanded و پانصد کامنت not recommanded  را در نظر بگیرید. فرض کنید الگوریتم هر هزار کامنت را recommanded برچسب گذاری کند. در این صورت معیار recall برابر ۱۰۰ خواهد شد در صورتی که پیشبینی نیمی از داده ها اشتباه صورت گرفته است.\n",
    "    <br>\n",
    "     F1 برابر میانگین همساز دو حالت قبل است. همانطور که میدانیم این میانگین هنگامی کاربرد دارد که محاسبه میانگین نرخ ها اهمیت داشته باشد. در اصل بهترین میانگینی که میتوانیم داشته باشیم و میتواند ارائه خوبی از مدل ما باشد\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre Process, No Additive Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 91.125000\n",
      "Precision: 87.643021\n",
      "Recall: 95.750000\n",
      "F1: 91.517324\n"
     ]
    }
   ],
   "source": [
    "from __future__ import unicode_literals\n",
    "from hazm import *\n",
    "import pandas as pd\n",
    "import math\n",
    "import csv\n",
    "\n",
    "REC = 0\n",
    "NOT_REC = 1\n",
    "trained = [dict(), dict()]\n",
    "\n",
    "def read_and_tokenize(path):\n",
    "    file_content = pd.read_csv(path)\n",
    "    file_content.replace('not_recommended', NOT_REC, inplace = True)\n",
    "    file_content.replace('recommended', REC, inplace = True)\n",
    "    train_data = file_content.values.tolist()\n",
    "    for i in train_data:\n",
    "        i[0] = word_tokenize(i[0])\n",
    "        i[1] = word_tokenize(i[1])\n",
    "        i[1] = i[1] + i[0]\n",
    "    return train_data\n",
    "\n",
    "train_data = read_and_tokenize(\"comment_train.csv\")\n",
    "\n",
    "# stemming and lemmatizing\n",
    "stemmer = Stemmer()\n",
    "lemmatizer = Lemmatizer()\n",
    "for comment in train_data:\n",
    "    for i in range(len(comment[1])):\n",
    "        comment[1][i] = stemmer.stem(comment[1][i])\n",
    "        comment[1][i] = lemmatizer.lemmatize(comment[1][i])\n",
    "\n",
    "# read stopwords from file\n",
    "stopwords = []\n",
    "with open ('persian_stop_words.txt', 'r') as f:\n",
    "    first_column = [row[0] for row in csv.reader(f,delimiter='\\t')]\n",
    "    stopwords += first_column[1:]\n",
    "\n",
    "# remove stopwords\n",
    "for comment in train_data:\n",
    "    for i in range(len(comment[1]) - 1, -1, -1):\n",
    "        if comment[1][i] in stopwords:\n",
    "            comment[1].pop(i)\n",
    "            \n",
    "\n",
    "# count number of words and split recommand and not recommand\n",
    "size = [0, 0]\n",
    "for i in train_data:\n",
    "    rec_st = i[2]\n",
    "    for j in i[1]:\n",
    "        if j in trained[rec_st]: trained[rec_st][j] += 1\n",
    "        else: trained[rec_st][j] = 1\n",
    "        size[rec_st] += 1\n",
    "\n",
    "# use log(probality) to increase presicion.\n",
    "for i in trained[REC]:\n",
    "    trained[REC][i] = math.log(trained[REC][i] / size[REC])\n",
    "for i in trained[NOT_REC]:\n",
    "    trained[NOT_REC][i] = math.log(trained[NOT_REC][i] / size[NOT_REC])\n",
    "\n",
    "rec_notrec_prob = [math.log(size[REC] / (size[REC] + size[NOT_REC])), math.log(size[NOT_REC] / (size[REC] + size[NOT_REC]))]\n",
    "\n",
    "test_data = read_and_tokenize(\"comment_test.csv\")\n",
    "\n",
    "for comment in test_data:\n",
    "    for i in range(len(comment[1])):\n",
    "        comment[1][i] = stemmer.stem(comment[1][i])\n",
    "        comment[1][i] = lemmatizer.lemmatize(comment[1][i])\n",
    "\n",
    "for comment in test_data:\n",
    "    for i in range(len(comment[1]) - 1, -1, -1):\n",
    "        if comment[1][i] in stopwords: comment[1].pop(i)\n",
    "        \n",
    "result = []\n",
    "for comment in test_data:\n",
    "    rec_prob = rec_notrec_prob[REC]\n",
    "    not_rec_prob = rec_notrec_prob[NOT_REC]\n",
    "    for word in comment[1]:\n",
    "        if (word in trained[REC]) and (word in trained[NOT_REC]):\n",
    "            rec_prob = rec_prob + trained[REC][word]\n",
    "            not_rec_prob = not_rec_prob + trained[NOT_REC][word]\n",
    "        elif (word in trained[REC]) and (word not in trained[NOT_REC]):\n",
    "            rec_prob = rec_prob + trained[REC][word]\n",
    "            not_rec_prob = float('-inf')\n",
    "        elif (word not in trained[REC]) and (word in trained[NOT_REC]):\n",
    "            rec_prob = float('-inf')\n",
    "            not_rec_prob = not_rec_prob + trained[NOT_REC][word]\n",
    "        elif (word not in trained[REC]) and (word not in trained[NOT_REC]):\n",
    "             rec_prob = rec_prob + math.log(0.5)\n",
    "             not_rec_prob = not_rec_prob + math.log(0.5)\n",
    "    if rec_prob < not_rec_prob: result.append(NOT_REC)\n",
    "    else: result.append(REC)\n",
    "        \n",
    "corrects = 0\n",
    "correct_decected_rec = 0\n",
    "all_detected_rec = 0\n",
    "total_rec = 0\n",
    "for i in range(len(test_data)):\n",
    "    if test_data[i][2] == result[i]:\n",
    "        corrects += 1\n",
    "    if result[i] == REC:\n",
    "        all_detected_rec += 1\n",
    "        if test_data[i][2] == REC:\n",
    "            correct_decected_rec += 1\n",
    "    if test_data[i][2] == REC:\n",
    "        total_rec += 1\n",
    "precision = (correct_decected_rec / all_detected_rec) * 100\n",
    "recall = (correct_decected_rec / total_rec) * 100\n",
    "print(\"Accuracy: %f\" % ((corrects / len(test_data)) * 100))\n",
    "print(\"Precision: %f\" % precision)\n",
    "print(\"Recall: %f\" % recall)\n",
    "print(\"F1: %f\" % ((2*precision*recall)/(precision+recall)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\" style=\"color:black;font-size:18px;\">\n",
    "در نتیجه بالا سه کار انجام داده ایم:\n",
    "    <br>\n",
    "اول)‌ stop words حذف شده است\n",
    "    <br>\n",
    "دوم) stemming صورت گرفته است.\n",
    "    <br>\n",
    "سوم) lemmatization صورت گرفته است.\n",
    "</div>\n",
    "<div dir=\"rtl\" style=\"color:black;font-size:18px;\">\n",
    "مشاهده میشود نسبت به حالتی که هیچ پیش پردازشی انجام نشده است ۰.۵ درصد بهبود حاصل شده است\n",
    "</div>\n",
    "<div dir=\"rtl\" style=\"color:black;font-size:18px;\">\n",
    "حذف stop words ها ضروری است زیرا زبان فارسی به گونه ای است که میتوان تعداد زیادی از آنها را بدون تغییر معنی به کاربرد و کلا این نوع کلمات تاثیری در تصمیم گیری ما نباید داشته باشد.\n",
    "    <br>\n",
    "اگر در کلمات stemming انجام ندهیم مقدار accuracy برابر ۹۰.۵ خواهد بود \n",
    "    <br>\n",
    "اگر در کلمات lemmatization انجام ندهیم مقدار accuracy برابر ۹۰.۷۵ خواهد بود\n",
    "    <br>\n",
    "    اگر هیچ کدام از این دوکار را انجام ندهیم دقت به ۹۰.۳۷ خواهد رسید\n",
    "    <br>\n",
    "پس بهتر است هر دوی این کارها را انجام دهیم\n",
    "<br>\n",
    "    همچنین توجه کنید نیازی به نرمالایز کردن جملات نیست. دلیل اول آنکه بعضی کلمات مانند «جوش» به اشتباه به «جو» تعبیر میشوند و این اشکال از خود کتابخانه هضم است و دلیل دوم آنکه به عنوان مثال اگر نشانه های جمع بدون نیم فاصله نوشته شوند در قسمت حذف کردن استاپ وردها و اگر با نیم فاصله نوشته شوند در قسمت stemming حذف میشوند.\n",
    "    <br>\n",
    "    همچنین در این قسمت از استاپ وردهای خود هضم استفاده نکرده ایم. زیرا دقت پایین می آمد. از استاپ وردهایی که در اینترنت وجود داشت استفاده شده است.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# No Pre Process, Additive Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 93.750000\n",
      "Precision: 92.892157\n",
      "Recall: 94.750000\n",
      "F1: 93.811881\n"
     ]
    }
   ],
   "source": [
    "from __future__ import unicode_literals\n",
    "from hazm import *\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "REC = 0\n",
    "NOT_REC = 1\n",
    "trained = [dict(), dict()]\n",
    "size = [0, 0]\n",
    "\n",
    "def read_and_tokenize(path):\n",
    "    file_content = pd.read_csv(path)\n",
    "    file_content.replace('not_recommended', NOT_REC, inplace = True)\n",
    "    file_content.replace('recommended', REC, inplace = True)\n",
    "    train_data = file_content.values.tolist()\n",
    "    normalizer = Normalizer()\n",
    "    for i in train_data:\n",
    "        i[0] = word_tokenize(i[0])\n",
    "        i[1] = word_tokenize(i[1])\n",
    "        i[1] = i[1] + i[0]\n",
    "    return train_data\n",
    "\n",
    "train_data = read_and_tokenize(\"comment_train.csv\")\n",
    "\n",
    "for i in train_data:\n",
    "    rec_st = i[2]\n",
    "    for j in i[1]:\n",
    "        if j in trained[rec_st]: trained[rec_st][j] += 1\n",
    "        else: trained[rec_st][j] = 1\n",
    "        size[rec_st] += 1\n",
    "\n",
    "# additive smoothing\n",
    "temp = []\n",
    "for comment in trained[REC]:\n",
    "    if comment not in trained[NOT_REC]:\n",
    "        temp.append(comment)\n",
    "if len(temp) != 0:\n",
    "    for comment in trained[NOT_REC]:\n",
    "        trained[NOT_REC][comment] += 1\n",
    "    for t in temp:\n",
    "        trained[NOT_REC][t] = 1\n",
    "temp = []\n",
    "for comment in trained[NOT_REC]:\n",
    "    if comment not in trained[REC]:\n",
    "        temp.append(comment)\n",
    "if len(temp) != 0:\n",
    "    for comment in trained[REC]:\n",
    "        trained[REC][comment] += 1\n",
    "    for t in temp:\n",
    "        trained[REC][t] = 1\n",
    "    \n",
    "\n",
    "for i in trained[REC]:\n",
    "    trained[REC][i] = math.log(trained[REC][i] / size[REC])\n",
    "\n",
    "for i in trained[NOT_REC]:\n",
    "    trained[NOT_REC][i] = math.log(trained[NOT_REC][i] / size[NOT_REC])\n",
    "\n",
    "rec_notrec_prob = [math.log(size[REC] / (size[REC] + size[NOT_REC])), math.log(size[NOT_REC] / (size[REC] + size[NOT_REC]))]\n",
    "\n",
    "test_data = read_and_tokenize(\"comment_test.csv\")\n",
    "\n",
    "result = []\n",
    "for comment in test_data:\n",
    "    rec_prob = rec_notrec_prob[REC]\n",
    "    not_rec_prob = rec_notrec_prob[NOT_REC]\n",
    "    for word in comment[1]:\n",
    "        if (word in trained[REC]) and (word in trained[NOT_REC]):\n",
    "            rec_prob = rec_prob + trained[REC][word]\n",
    "            not_rec_prob = not_rec_prob + trained[NOT_REC][word]\n",
    "        elif (word not in trained[REC]) and (word not in trained[NOT_REC]):\n",
    "             rec_prob = rec_prob + math.log(0.5)\n",
    "             not_rec_prob = not_rec_prob + math.log(0.5)\n",
    "    if rec_prob < not_rec_prob: result.append(NOT_REC)\n",
    "    else: result.append(REC)\n",
    "        \n",
    "corrects = 0\n",
    "correct_decected_rec = 0\n",
    "all_detected_rec = 0\n",
    "total_rec = 0\n",
    "for i in range(len(test_data)):\n",
    "    if test_data[i][2] == result[i]:\n",
    "        corrects += 1\n",
    "    if result[i] == REC:\n",
    "        all_detected_rec += 1\n",
    "        if test_data[i][2] == REC:\n",
    "            correct_decected_rec += 1\n",
    "    if test_data[i][2] == REC:\n",
    "        total_rec += 1\n",
    "precision = (correct_decected_rec / all_detected_rec) * 100\n",
    "recall = (correct_decected_rec / total_rec) * 100\n",
    "print(\"Accuracy: %f\" % ((corrects / len(test_data)) * 100))\n",
    "print(\"Precision: %f\" % precision)\n",
    "print(\"Recall: %f\" % recall)\n",
    "print(\"F1: %f\" % ((2*precision*recall)/(precision+recall)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\" style=\"color:black;font-size:18px;\">\n",
    "در حالت بدون additive smoothing اگر کلمه ای جزو یکی از دسته های recommended و not recommended باشد و جزو دیگری نباشد \n",
    "تکلیف کامنت به صورت قطعی مشخص میشود. به عنوان مثال اگر کلمه دیجی کالا فقط در دسته recommended باشد و در دسته not recommended نباشد الگوریتم کل کامنت را به صورت recommended برچسب گذاری میکند\n",
    "<br>\n",
    "    در اصل این اتفاق وقتی می افتد که در الگوریتم وقتی کلمه ای وجود نداشت جای لگاریتم احتمال آن منفی بی نهایت گذاشتیم\n",
    "    <br>\n",
    "    برای حل این مشکل از additive smoothing استفاده میکنیم. به این صورت که کلماتی که فقط در یک گروه هستند را در یک لیست ذخیره میکنیم. سپس آنها را به گروهی که در آن نیستند با وزن یک اضافه میکنیم و به سایر کلمات نیز یک واحد اضافه میکنیم. این کار باعث میشود وقتی کلمه ای در یک گروه وجود ندارد باعث نشود کل کامنت به صورت قطعی عضو یک گروه شود. مشاهده میشود در این روش نسبت به حالت عادی ۳.۱ درصد افزایش دقت داریم.\n",
    "<br>\n",
    "    توجه کنید که عملیات ذکر شده additive smoothing به ازای آلفای یک است.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre Process, Additive Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BAD COMMENT\n",
      "خرید یه مد کار بکنه مشخص میشه کیف قطعات ور گود\n",
      "BAD COMMENT\n",
      "این قیم گزینه  بهتر ه میشه گرف \r",
      " رو مینویسه ول زیاد مناسب نیس رنگ میده یه وقتا موقع نوشتن زیاد مناسب نیس رنگ میده یه وقتا موقع نوشتن\n",
      "BAD COMMENT\n",
      "سلا راح کابل شارژ توصیه میشود ارز گوش شارژ وایرلس مجهز کنید نقد خرید\n",
      "BAD COMMENT\n",
      "جزو افراد سیزده ساله انواع فیل سرک اع روغن هوا اتاق استفاده میکرد ول تازگ متوجه اطلاع یاف فیل گاج باکیف  فیل سرک می‌باشد ه چنین قیم بمراتب مناسب ترب نسب سرک طرف فروشنده دا روغن فیلتر می‌فروخ واقعا اثب کرد گاج باکیف  سرک می‌باشد بررس فیل سرک\n",
      "BAD COMMENT\n",
      "سلا دوس استفاده چراغ چک تویوتا کمر ۲۰۰۷ خامو چراغ چک موتور خامو\n",
      "Accuracy: 94.125000\n",
      "Precision: 93.366093\n",
      "Recall: 95.000000\n",
      "F1: 94.175960\n"
     ]
    }
   ],
   "source": [
    "from __future__ import unicode_literals\n",
    "from hazm import *\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "REC = 0\n",
    "NOT_REC = 1\n",
    "trained = [dict(), dict()]\n",
    "size = [0, 0]\n",
    "\n",
    "def read_and_tokenize(path):\n",
    "    file_content = pd.read_csv(path)\n",
    "    file_content.replace('not_recommended', NOT_REC, inplace = True)\n",
    "    file_content.replace('recommended', REC, inplace = True)\n",
    "    train_data = file_content.values.tolist()\n",
    "    normalizer = Normalizer()\n",
    "    for i in train_data:\n",
    "        i[0] = word_tokenize(i[0])\n",
    "        i[1] = word_tokenize(i[1])\n",
    "        i[1] = i[1] + i[0]\n",
    "    return train_data\n",
    "\n",
    "train_data = read_and_tokenize(\"comment_train.csv\")\n",
    "\n",
    "# stemming and lemmatizing\n",
    "stemmer = Stemmer()\n",
    "lemmatizer = Lemmatizer()\n",
    "for comment in train_data:\n",
    "    for i in range(len(comment[1])):\n",
    "        comment[1][i] = stemmer.stem(comment[1][i])\n",
    "        #comment[1][i] = lemmatizer.lemmatize(comment[1][i])\n",
    "        pass\n",
    "\n",
    "# read stopwords from file\n",
    "stopwords = []\n",
    "with open ('persian_stop_words.txt', 'r') as f:\n",
    "    first_column = [row[0] for row in csv.reader(f,delimiter='\\t')]\n",
    "    stopwords += first_column[1:]\n",
    "\n",
    "# remove stopwords\n",
    "for comment in train_data:\n",
    "    for i in range(len(comment[1]) - 1, -1, -1):\n",
    "        if comment[1][i] in stopwords:\n",
    "            comment[1].pop(i)\n",
    "\n",
    "for i in train_data:\n",
    "    rec_st = i[2]\n",
    "    for j in i[1]:\n",
    "        if j in trained[rec_st]: trained[rec_st][j] += 1\n",
    "        else: trained[rec_st][j] = 1\n",
    "        size[rec_st] += 1\n",
    "\n",
    "# additive smoothing\n",
    "temp = []\n",
    "for comment in trained[REC]:\n",
    "    if comment not in trained[NOT_REC]:\n",
    "        temp.append(comment)\n",
    "if len(temp) != 0:\n",
    "    for comment in trained[NOT_REC]:\n",
    "        trained[NOT_REC][comment] += 1\n",
    "    for t in temp:\n",
    "        trained[NOT_REC][t] = 1\n",
    "temp = []\n",
    "for comment in trained[NOT_REC]:\n",
    "    if comment not in trained[REC]:\n",
    "        temp.append(comment)\n",
    "if len(temp) != 0:\n",
    "    for comment in trained[REC]:\n",
    "        trained[REC][comment] += 1\n",
    "    for t in temp:\n",
    "        trained[REC][t] = 1\n",
    "    \n",
    "\n",
    "for i in trained[REC]:\n",
    "    trained[REC][i] = math.log(trained[REC][i] / size[REC])\n",
    "\n",
    "for i in trained[NOT_REC]:\n",
    "    trained[NOT_REC][i] = math.log(trained[NOT_REC][i] / size[NOT_REC])\n",
    "\n",
    "rec_notrec_prob = [math.log(size[REC] / (size[REC] + size[NOT_REC])), math.log(size[NOT_REC] / (size[REC] + size[NOT_REC]))]\n",
    "\n",
    "test_data = read_and_tokenize(\"comment_test.csv\")\n",
    "\n",
    "# stemming and lemmatizing\n",
    "for comment in test_data:\n",
    "    for i in range(len(comment[1])):\n",
    "        comment[1][i] = stemmer.stem(comment[1][i])\n",
    "        #comment[1][i] = lemmatizer.lemmatize(comment[1][i])\n",
    "    \n",
    "# remove stopwords\n",
    "for comment in test_data:\n",
    "    for i in range(len(comment[1]) - 1, -1, -1):\n",
    "        if comment[1][i] in stopwords: comment[1].pop(i)\n",
    "\n",
    "result = []\n",
    "for comment in test_data:\n",
    "    rec_prob = rec_notrec_prob[REC]\n",
    "    not_rec_prob = rec_notrec_prob[NOT_REC]\n",
    "    for word in comment[1]:\n",
    "        if (word in trained[REC]) and (word in trained[NOT_REC]):\n",
    "            rec_prob = rec_prob + trained[REC][word]\n",
    "            not_rec_prob = not_rec_prob + trained[NOT_REC][word]\n",
    "        elif (word not in trained[REC]) and (word not in trained[NOT_REC]):\n",
    "             rec_prob = rec_prob + math.log(0.5)\n",
    "             not_rec_prob = not_rec_prob + math.log(0.5)\n",
    "    if rec_prob < not_rec_prob: result.append(NOT_REC)\n",
    "    else: result.append(REC)\n",
    "        \n",
    "corrects = 0\n",
    "correct_decected_rec = 0\n",
    "all_detected_rec = 0\n",
    "total_rec = 0\n",
    "c = 0\n",
    "for i in range(len(test_data)):\n",
    "    if test_data[i][2] == result[i]:\n",
    "        corrects += 1\n",
    "    if result[i] == REC:\n",
    "        all_detected_rec += 1\n",
    "        if test_data[i][2] == REC:\n",
    "            correct_decected_rec += 1\n",
    "    if test_data[i][2] == REC:\n",
    "        total_rec += 1\n",
    "    if c < 5 and test_data[i][2] != result[i]:\n",
    "        print(\"BAD COMMENT\")\n",
    "        print(' '.join(map(str, test_data[i][1])) )\n",
    "        c += 1\n",
    "precision = (correct_decected_rec / all_detected_rec) * 100\n",
    "recall = (correct_decected_rec / total_rec) * 100\n",
    "print(\"Accuracy: %f\" % ((corrects / len(test_data)) * 100))\n",
    "print(\"Precision: %f\" % precision)\n",
    "print(\"Recall: %f\" % recall)\n",
    "print(\"F1: %f\" % ((2*precision*recall)/(precision+recall)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\" style=\"color:black;font-size:18px;\">\n",
    "     توجه کنید در حالت بالا عملیات lemmatize را حذف کردیم زیرا باعث میشد دقت نسبت به حالتی pre process نداریم کاهش \n",
    "    پیدا کند \n",
    "<br>\n",
    "    پنج کامنتی که به اشتباه تشخیص داده شده اند آورده شده است. (تایتل و متن باهم) یکی از دلایلی که ممکن است اشتباه تشخیص داده شود تایتل های نامربوط است. دلیل دیگر میتواند مشخص نبودن پیشنهاد دادن یا ندادن باشد. به عنوان مثال برنامه کامنت آخر را پیشنهاد نشده در نظر گرفته که در اصل پیشنهاد شده بوده است. تشخیص اینکه این کامنت پیشنهاد شده یا نشده برای انسان نیز دشوار است و عدم قطعیت دارد. دلیل دیگر میتواند stem کردن کلمات باشد که آنها را عوض میکند. مثلا سلام به سلا تبدیل شده است. دلیل دیگر میتواند ترکیب کلمات مثبت و منفی در یک کامنت باشد که سخت میشود تشخیص داد آن کامنت مثبت بوده یا منفی. دلیل دیگر ندانستن محصول مورد نظر است. مثلا در کامنت چهارم گفته میشود که گاج با کیفیت تر از سرکان است. اگر این کامنت برای گاج گذاشته شده باشد مثبت و اگر برای سرکان گذاشته شده باشد منفی است.\n",
    "دلیل دیگر میتواند نزدیک بودن دو احتمال recommended بودن و not recommended بودن است که با توجه به داده های train شده ممکن است متفاوت شود. اگر این دو احتمال نزدیک به هم باشد تصمیم گیری سخت میشود.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "  <tr>\n",
    "    <th></th>\n",
    "    <th>No Pre Process, No Additive Smoothing</th>\n",
    "    <th>Pre Process, No Additive Smoothing</th>\n",
    "    <th>No Pre Process, Additive Smoothing</th>\n",
    "    <th>Pre Process, Additive Smoothing</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Accuracy</td>\n",
    "    <td>90.625000</td>\n",
    "    <td>91.125000</td>\n",
    "    <td>93.750000</td>\n",
    "    <td>94.125000</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Precision</td>\n",
    "    <td>86.848073</td>\n",
    "    <td>87.643021</td>\n",
    "    <td>92.892157</td>\n",
    "    <td>93.366093</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Recall</td>\n",
    "    <td>95.750000</td>\n",
    "    <td>95.750000</td>\n",
    "    <td>94.750000</td>\n",
    "    <td>95.000000</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>F1</td>\n",
    "    <td>91.082045</td>\n",
    "    <td>91.517324</td>\n",
    "    <td>93.811881</td>\n",
    "    <td>94.175960</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\" style=\"color:black;font-size:18px;\">\n",
    "    pre process آزاردهنده است. ممکن است تصور کنیم یکی از کارهایی که در راستای pre process میکنیم مفید است اما باید تست شود و هر pre process ای به بهتر شدن نتیجه کمک نمیکند.\n",
    "    <br>\n",
    "    additive smoothing همواره کمک میکند و باعث میشود دقت بیشتر شود.\n",
    "    <br>\n",
    "    اگر از هر دوی روشهای pre process و smoothing استفاده کنیم دقت از حالات دیگر بیشتر میشود. اما در این حالت نیز باید چک شود چه نوع pre process ای نتایج مارا بهتر میکند. به عنوان مثال در حالتی که smoothing نداشتیم استفاده از هر دوی stemming و lemmatize باعث بیشینه شدن دقت میشود اما در این حالت اگر از lemmatize استفاده نکنیم دقت بیشتر میشود و باید همه را تست کرد\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
